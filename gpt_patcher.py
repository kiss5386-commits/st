#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import json
import yaml
import subprocess
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import openai

# ===== ì„¤ì • =====
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5-mini")  # í™˜ê²½ë³€ìˆ˜ë¡œ ëª¨ë¸ ì„ íƒ
MAX_CONTEXT_TOKENS = 200000     # GPT-5 ê¸°ì¤€ ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸
MAX_COMPLETION_TOKENS = 16000   # GPT-5 ì‘ë‹µ í† í° í™•ëŒ€
RETRY_ATTEMPTS = 3
RETRY_DELAY = 2

# ëª¨ë¸ë³„ í† í° ì œí•œ ì„¤ì •
MODEL_LIMITS = {
    "gpt-5": {"context": 200000, "completion": 16000},
    "gpt-5-mini": {"context": 200000, "completion": 8000},
    "gpt-5-nano": {"context": 128000, "completion": 4000},
    "gpt-4o": {"context": 120000, "completion": 8000}, 
    "gpt-4o-mini": {"context": 120000, "completion": 8000},
    "o1-preview": {"context": 120000, "completion": 32000},
    "o1-mini": {"context": 120000, "completion": 16000}
}

# ì¶”ì í•  íŒŒì¼ íŒ¨í„´
TRACKED_PATTERNS = [
    "*.py", "*.js", "*.ts", "*.html", "*.css", "*.json", "*.yml", "*.yaml",
    "*.md", "*.txt", "*.sh", "*.bat", "*.sql", "*.env", "*.ini", "*.cfg"
]

# ë¬´ì‹œí•  ë””ë ‰í† ë¦¬/íŒŒì¼
IGNORE_PATTERNS = [
    ".git", "__pycache__", "node_modules", ".venv", "venv", 
    "build", "dist", ".pytest_cache", ".idea", ".vscode"
]

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class TokenManager:
    """í† í° ì‚¬ìš©ëŸ‰ ê´€ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ ìµœì í™”"""

    @staticmethod
    def truncate_content(content: str, max_tokens: int) -> str:
        """ì»¨í…ì¸ ë¥¼ í† í° ì œí•œì— ë§ê²Œ ì˜ë¼ëƒ„"""
        estimated_tokens = TokenManager.estimate_tokens(content)
        if estimated_tokens <= max_tokens:
            return content

        # ëŒ€ëµì  ë¹„ìœ¨ë¡œ ì˜ë¼ë‚´ê¸°(+ì•ˆì „ ë§ˆì§„)
        ratio = max_tokens / max(estimated_tokens, 1)
        truncate_length = int(len(content) * ratio * 0.9)

        truncated = content[:truncate_length]
        return truncated + "\n\n[... ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ ìƒëµë¨ ...]"
    
    @staticmethod
    def estimate_tokens(text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (1í† í° â‰ˆ 4ê¸€ì)"""
        return len(text) // 3  # ë³´ìˆ˜ì  ì¶”ì •
    
    @staticmethod
    def extract_code_structure(content: str, max_tokens: int) -> str:
        """ëŒ€ìš©ëŸ‰ ì½”ë“œì—ì„œ í•µì‹¬ êµ¬ì¡°ë§Œ ì¶”ì¶œ"""
        lines = content.split('\n')
        extracted_lines = []
        current_tokens = 0
        
        # ìš°ì„ ìˆœìœ„ë³„ ì¶”ì¶œ
        priorities = [
            # 1ìˆœìœ„: í´ë˜ìŠ¤/í•¨ìˆ˜ ì •ì˜
            (r'^class\s+\w+', 'í´ë˜ìŠ¤ ì •ì˜'),
            (r'^def\s+\w+', 'í•¨ìˆ˜ ì •ì˜'),
            (r'^\s+def\s+\w+', 'ë©”ì„œë“œ ì •ì˜'),
            # 2ìˆœìœ„: ì¤‘ìš” ì„¤ì •/ìƒìˆ˜
            (r'^[A-Z_]+=', 'ìƒìˆ˜ ì •ì˜'),
            (r'^import\s+|^from\s+', 'ì„í¬íŠ¸'),
            # 3ìˆœìœ„: ì£¼ì„ê³¼ ë…ìŠ¤íŠ¸ë§
            (r'^\s*"""', 'ë…ìŠ¤íŠ¸ë§'),
            (r'^\s*#.*ì¤‘ìš”|^\s*#.*TODO|^\s*#.*FIXME', 'ì¤‘ìš” ì£¼ì„'),
        ]
        
        import re
        
        # ìš°ì„ ìˆœìœ„ë³„ ì¶”ì¶œ
        for pattern, desc in priorities:
            if current_tokens >= max_tokens * 0.9:
                break
                
            for i, line in enumerate(lines):
                if re.match(pattern, line):
                    # í•¨ìˆ˜/í´ë˜ìŠ¤ì˜ ê²½ìš° ì‹œê·¸ë‹ˆì²˜ì™€ ë…ìŠ¤íŠ¸ë§ í¬í•¨
                    if 'def ' in line or 'class ' in line:
                        block = TokenManager.extract_function_block(lines, i)
                        block_tokens = TokenManager.estimate_tokens('\n'.join(block))
                        
                        if current_tokens + block_tokens <= max_tokens:
                            extracted_lines.extend(block)
                            current_tokens += block_tokens
                    else:
                        line_tokens = TokenManager.estimate_tokens(line)
                        if current_tokens + line_tokens <= max_tokens:
                            extracted_lines.append(line)
                            current_tokens += line_tokens
        
        if not extracted_lines:
            # êµ¬ì¡° ì¶”ì¶œ ì‹¤íŒ¨ì‹œ ì•ë¶€ë¶„ë§Œ
            truncated = TokenManager.truncate_content(content, max_tokens)
            return truncated
        
        result = '\n'.join(extracted_lines)
        result += f"\n\n# ... ì´ {len(lines)}ì¤„ ì¤‘ í•µì‹¬ êµ¬ì¡° {len(extracted_lines)}ì¤„ ì¶”ì¶œë¨ ..."
        return result
    
    @staticmethod
    def extract_function_block(lines: List[str], start_idx: int) -> List[str]:
        """í•¨ìˆ˜/í´ë˜ìŠ¤ ë¸”ë¡ ì¶”ì¶œ (ì‹œê·¸ë‹ˆì²˜ + ë…ìŠ¤íŠ¸ë§ë§Œ)"""
        block = [lines[start_idx]]  # í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜ ë¼ì¸
        
        i = start_idx + 1
        indent_level = len(lines[start_idx]) - len(lines[start_idx].lstrip())
        
        # ë…ìŠ¤íŠ¸ë§ì´ë‚˜ ì£¼ìš” ì£¼ì„ë§Œ í¬í•¨
        in_docstring = False
        docstring_quotes = None
        
        while i < len(lines) and i < start_idx + 20:  # ìµœëŒ€ 20ì¤„ê¹Œì§€ë§Œ
            line = lines[i]
            
            if not line.strip():  # ë¹ˆ ì¤„
                block.append(line)
                i += 1
                continue
                
            line_indent = len(line) - len(line.lstrip())
            
            # ë“¤ì—¬ì“°ê¸°ê°€ ì›ë˜ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìœ¼ë©´ í•¨ìˆ˜ ë
            if line_indent <= indent_level and line.strip():
                break
            
            # ë…ìŠ¤íŠ¸ë§ ì²˜ë¦¬
            if '"""' in line or "'''" in line:
                if not in_docstring:
                    docstring_quotes = '"""' if '"""' in line else "'''"
                    in_docstring = True
                    block.append(line)
                elif docstring_quotes in line:
                    block.append(line)
                    in_docstring = False
                    break  # ë…ìŠ¤íŠ¸ë§ ëë‚˜ë©´ í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ì™„ë£Œ
                else:
                    block.append(line)
            elif in_docstring:
                block.append(line)
            elif line.strip().startswith('#'):  # ì£¼ì„
                block.append(line)
            elif 'return ' in line or 'yield ' in line:  # ë°˜í™˜ íƒ€ì… íŒíŠ¸
                block.append(line)
                break
            
            i += 1
        
        # í•¨ìˆ˜ ë³¸ë¬¸ ìƒëµ í‘œì‹œ
        if i < len(lines) and i > start_idx + 1:
            block.append(" " * (indent_level + 4) + "# ... í•¨ìˆ˜ ë³¸ë¬¸ ìƒëµ ...")
            
        return block
    
    @staticmethod
    def optimize_file_list(files: List[Dict], max_tokens: int) -> List[Dict]:
        """íŒŒì¼ ëª©ë¡ì„ í† í° ì œí•œì— ë§ê²Œ ìµœì í™” - ëŒ€ìš©ëŸ‰ ë‹¨ì¼íŒŒì¼ ì§€ì›"""
        total_tokens = 0
        optimized_files = []
        
        # ìš°ì„ ìˆœìœ„: .py íŒŒì¼ ë¨¼ì €, ê·¸ ë‹¤ìŒ í¬ê¸°ìˆœ
        def file_priority(f):
            path = f.get('path', '')
            if 'stargate' in path and path.endswith('.py'):
                return 0  # ìµœê³  ìš°ì„ ìˆœìœ„
            elif path.endswith('.py'):
                return 1
            else:
                return 2
        
        sorted_files = sorted(files, key=lambda f: (file_priority(f), len(f.get('content', ''))))
        
        for file_info in sorted_files:
            content = file_info.get('content', '')
            file_tokens = TokenManager.estimate_tokens(content)
            path = file_info.get('path', '')
            
            # ëŒ€ìš©ëŸ‰ ë‹¨ì¼ íŒŒì¼ íŠ¹ë³„ ì²˜ë¦¬
            if file_tokens > max_tokens * 0.8:  # ì „ì²´ í† í°ì˜ 80% ì´ìƒ
                logger.info(f"ğŸ”¥ ëŒ€ìš©ëŸ‰ íŒŒì¼ ê°ì§€: {path} ({file_tokens:,} í† í°)")
                
                # í•µì‹¬ ë¶€ë¶„ë§Œ ì¶”ì¶œ (í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜, í´ë˜ìŠ¤ ì •ì˜, ì£¼ìš” ë¡œì§)
                optimized_content = TokenManager.extract_code_structure(content, max_tokens)
                file_info['content'] = optimized_content
                file_info['is_large_file'] = True
                optimized_files.append(file_info)
                total_tokens += TokenManager.estimate_tokens(optimized_content)
                
                logger.info(f"ğŸ“ ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™”: {file_tokens:,} â†’ {TokenManager.estimate_tokens(optimized_content):,} í† í°")
                break  # ëŒ€ìš©ëŸ‰ íŒŒì¼ í•˜ë‚˜ë§Œ ì²˜ë¦¬
            
            # ì¼ë°˜ íŒŒì¼ ì²˜ë¦¬
            elif total_tokens + file_tokens <= max_tokens:
                optimized_files.append(file_info)
                total_tokens += file_tokens
            else:
                # ë‚¨ì€ ê³µê°„ìœ¼ë¡œ ì¶•ì•½
                remaining_tokens = max_tokens - total_tokens
                if remaining_tokens > 500:  # ìµœì†Œ 500í† í°
                    file_info['content'] = TokenManager.truncate_content(content, remaining_tokens)
                    optimized_files.append(file_info)
                    total_tokens = max_tokens
                break
        
        logger.info(f"ğŸ“Š íŒŒì¼ ìµœì í™”: {len(files)} â†’ {len(optimized_files)}ê°œ, ì˜ˆìƒ í† í°: {total_tokens:,}")
        return optimized_files

class GitFileTracker:
    """Git ì €ì¥ì†Œ íŒŒì¼ ì¶”ì  ë° ê´€ë¦¬"""
    
    def __init__(self, repo_path: str = "."):
        self.repo_path = Path(repo_path)
        self.tracking_file = self.repo_path / ".gpt_tracking.json"
    
    def should_ignore(self, path: Path) -> bool:
        """íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ë¬´ì‹œ ëŒ€ìƒì¸ì§€ í™•ì¸"""
        path_str = str(path)
        return any(pattern in path_str for pattern in IGNORE_PATTERNS)
    
    def get_all_tracked_files(self) -> List[Path]:
        """ì¶”ì  ëŒ€ìƒ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        tracked_files = []
        
        for pattern in TRACKED_PATTERNS:
            for file_path in self.repo_path.rglob(pattern):
                if file_path.is_file() and not self.should_ignore(file_path):
                    tracked_files.append(file_path)
        
        return tracked_files
    
    def load_tracking_info(self) -> Dict:
        """ê¸°ì¡´ ì¶”ì  ì •ë³´ ë¡œë“œ"""
        if not self.tracking_file.exists():
            return {}
        
        try:
            with open(self.tracking_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ ì¶”ì  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def save_tracking_info(self, tracking_info: Dict):
        """ì¶”ì  ì •ë³´ ì €ì¥"""
        try:
            with open(self.tracking_file, 'w', encoding='utf-8') as f:
                json.dump(tracking_info, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ ì¶”ì  ì •ë³´ ì €ì¥ ì™„ë£Œ: {len(tracking_info)}ê°œ íŒŒì¼")
        except Exception as e:
            logger.error(f"âŒ ì¶”ì  ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_changed_files(self) -> List[Dict]:
        """ë³€ê²½ëœ íŒŒì¼ ëª©ë¡ê³¼ ë‚´ìš© ë°˜í™˜"""
        current_files = self.get_all_tracked_files()
        tracking_info = self.load_tracking_info()
        changed_files = []
        
        for file_path in current_files:
            try:
                # íŒŒì¼ ë‚´ìš© ì½ê¸°
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                relative_path = str(file_path.relative_to(self.repo_path))
                file_hash = hash(content)
                
                # ë³€ê²½ ì—¬ë¶€ í™•ì¸
                if relative_path not in tracking_info or tracking_info[relative_path] != file_hash:
                    changed_files.append({
                        'path': relative_path,
                        'content': content,
                        'size': len(content),
                        'is_new': relative_path not in tracking_info
                    })
                    tracking_info[relative_path] = file_hash
            
            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
        
        # ì¶”ì  ì •ë³´ ì—…ë°ì´íŠ¸
        self.save_tracking_info(tracking_info)
        return changed_files

class GPTPatcher:
    """GPT ê¸°ë°˜ ìë™ ì½”ë“œ íŒ¨ì¹˜"""
    
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.token_manager = TokenManager()
        self.model = OPENAI_MODEL
        
        # ëª¨ë¸ë³„ ì œí•œ ì„¤ì •
        if self.model in MODEL_LIMITS:
            self.max_context = MODEL_LIMITS[self.model]["context"] - 2000  # ì•ˆì „ ë§ˆì§„
            self.max_completion = MODEL_LIMITS[self.model]["completion"]
        else:
            # ê¸°ë³¸ê°’ (ì•ˆì „í•œ ì„¤ì •)
            self.max_context = 6000
            self.max_completion = 2000
            
        logger.info(f"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
        logger.info(f"ğŸ“Š í† í° ì œí•œ: Context={self.max_context}, Completion={self.max_completion}")
    
    def create_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return """ë‹¹ì‹ ì€ ì „ë¬¸ ì½”ë“œ ê°œë°œìì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:

1. **JSON í˜•ì‹ ì‘ë‹µ**: ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
```json
{
  "plan": "ìˆ˜í–‰í•  ì‘ì—… ê³„íš",
  "files": [
    {
      "path": "íŒŒì¼ê²½ë¡œ",
      "action": "create|modify|delete",
      "content": "ì „ì²´ íŒŒì¼ ë‚´ìš© (create/modifyì‹œ)",
      "reason": "ì‘ì—… ì´ìœ "
    }
  ],
  "summary": "ì‘ì—… ìš”ì•½"
}
```

2. **íŒŒì¼ ì²˜ë¦¬ ê·œì¹™**:
   - create: ìƒˆ íŒŒì¼ ìƒì„±
   - modify: ê¸°ì¡´ íŒŒì¼ ìˆ˜ì • (ì „ì²´ ë‚´ìš© ë®ì–´ì“°ê¸°)
   - delete: íŒŒì¼ ì‚­ì œ

3. **ì½”ë“œ í’ˆì§ˆ**:
   - ì—ëŸ¬ ì²˜ë¦¬ í•„ìˆ˜ (try/except)
   - ë¡œê¹… ì¶”ê°€
   - ì£¼ì„ìœ¼ë¡œ ë™ì‘ ì„¤ëª…
   - ê¸°ì¡´ ê¸°ëŠ¥ ë³´ì¡´

4. **ì•ˆì „ì¥ì¹˜**:
   - ì¤‘ìš” íŒŒì¼ ìˆ˜ì •ì‹œ ë°±ì—… ê³ ë ¤
   - í˜¸í™˜ì„± ìœ ì§€
   - í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ êµ¬ì¡°"""
    
    def create_user_prompt(self, instructions: str, files: List[Dict]) -> str:
        """ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # íŒŒì¼ ì •ë³´ ìš”ì•½
        files_summary = []
        for file_info in files:
            status = "ìƒˆ íŒŒì¼" if file_info.get('is_new') else "ìˆ˜ì •ë¨"
            size_kb = file_info['size'] / 1024
            files_summary.append(f"- {file_info['path']} ({status}, {size_kb:.1f}KB)")
        
        prompt = f"""## ì‘ì—… ì§€ì‹œë¬¸
{instructions}

## í˜„ì¬ íŒŒì¼ ìƒíƒœ
{chr(10).join(files_summary)}

## íŒŒì¼ ë‚´ìš©
"""
        
        # íŒŒì¼ ë‚´ìš© ì¶”ê°€
        for file_info in files[:10]:  # ìµœëŒ€ 10ê°œ íŒŒì¼ë§Œ í¬í•¨
            prompt += f"""
### {file_info['path']}
```
{file_info['content']}
```
"""
        
        return prompt
    
    def call_gpt_api(self, instructions: str, files: List[Dict]) -> Optional[Dict]:
        """GPT API í˜¸ì¶œ"""
        # ë™ì  í† í° ìµœì í™”
        available_tokens = self.max_context - 1000  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë“±ì„ ìœ„í•œ ì—¬ìœ 
        optimized_files = self.token_manager.optimize_file_list(files, available_tokens)
        
        system_prompt = self.create_system_prompt()
        user_prompt = self.create_user_prompt(instructions, optimized_files)
        
        # o1 ëª¨ë¸ ê³„ì—´ì€ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
        is_o1_model = self.model.startswith("o1")
        is_gpt5_model = self.model.startswith("gpt-5")
        
        for attempt in range(RETRY_ATTEMPTS):
            try:
                logger.info(f"ğŸ¤– GPT API í˜¸ì¶œ ì¤‘... (ì‹œë„ {attempt + 1}/{RETRY_ATTEMPTS})")
                
                if is_o1_model:
                    # o1 ëª¨ë¸ì€ system ë©”ì‹œì§€ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                    combined_prompt = f"{system_prompt}\n\n{user_prompt}"
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "user", "content": combined_prompt}
                        ],
                        max_completion_tokens=self.max_completion
                    )
                elif is_gpt5_model:
                    # GPT-5ëŠ” í–¥ìƒëœ íŒŒë¼ë¯¸í„° ì§€ì›
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        max_tokens=self.max_completion,
                        temperature=0.2,  # GPT-5ëŠ” ë” ë‚®ì€ ì˜¨ë„ë¡œ ì •í™•ì„± í–¥ìƒ
                        top_p=0.9
                    )
                else:
                    # ì¼ë°˜ ëª¨ë¸ (GPT-4o ë“±)
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        max_tokens=self.max_completion,
                        temperature=0.3
                    )
                
                content = response.choices[0].message.content.strip()
                
                # JSON íŒŒì‹± ì‹œë„
                try:
                    # ì½”ë“œ ë¸”ë¡ì´ ìˆë‹¤ë©´ ì œê±°
                    if "```json" in content:
                        content = content.split("```json")[1].split("```")[0].strip()
                    elif "```" in content:
                        content = content.split("```")[1].split("```")[0].strip()
                    
                    result = json.loads(content)
                    logger.info("âœ… GPT API í˜¸ì¶œ ì„±ê³µ")
                    return result
                
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                    logger.error(f"ì‘ë‹µ ë‚´ìš©: {content[:500]}...")
                    return None
            
            except Exception as e:
                logger.warning(f"âš ï¸ ì‹œë„ {attempt + 1}: LLM í˜¸ì¶œ ì‹¤íŒ¨ - {e}")
                if attempt < RETRY_ATTEMPTS - 1:
                    time.sleep(RETRY_DELAY)
        
        logger.error("âŒ ERROR: GPT API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨")
        return None
    
    def execute_file_operations(self, operations: List[Dict]) -> bool:
        """íŒŒì¼ ì‘ì—… ì‹¤í–‰"""
        success_count = 0
        
        for op in operations:
            try:
                path = Path(op['path'])
                action = op['action']
                
                if action == "create" or action == "modify":
                    # ë””ë ‰í† ë¦¬ ìƒì„±
                    path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # íŒŒì¼ ì“°ê¸°
                    with open(path, 'w', encoding='utf-8') as f:
                        f.write(op['content'])
                    
                    logger.info(f"âœ… {action}: {path}")
                    success_count += 1
                
                elif action == "delete":
                    if path.exists():
                        path.unlink()
                        logger.info(f"ğŸ—‘ï¸ ì‚­ì œ: {path}")
                        success_count += 1
                    else:
                        logger.warning(f"âš ï¸ ì‚­ì œ ëŒ€ìƒ ì—†ìŒ: {path}")
                
            except Exception as e:
                logger.error(f"âŒ íŒŒì¼ ì‘ì—… ì‹¤íŒ¨ {op['path']}: {e}")
        
        logger.info(f"ğŸ“Š íŒŒì¼ ì‘ì—… ì™„ë£Œ: {success_count}/{len(operations)} ì„±ê³µ")
        return success_count > 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        instructions = os.getenv("USER_INSTRUCTIONS", "")
        if not instructions:
            logger.error("âŒ USER_INSTRUCTIONS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        logger.info("ğŸš€ GPT ìë™ ì½”ë“œ ìˆ˜ì • ì‹œì‘")
        logger.info("=" * 50)
        logger.info(f"ğŸ“ ì‚¬ìš©ì ì§€ì‹œë¬¸: {instructions[:100]}...")
        
        # íŒŒì¼ ì¶”ì  ì‹œì‘
        tracker = GitFileTracker()
        changed_files = tracker.get_changed_files()
        
        if not changed_files:
            logger.info("ğŸ“‚ ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
            test_content = f"""# GPT ìë™ ìˆ˜ì • í…ŒìŠ¤íŠ¸
# ìƒì„± ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}
# ì§€ì‹œë¬¸: {instructions[:100]}...

print("GPT ìë™ ìˆ˜ì • í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
"""
            changed_files = [{
                'path': f'gpt_test_{int(time.time())}.py',
                'content': test_content,
                'size': len(test_content),
                'is_new': True
            }]
        
        logger.info(f"ğŸ“‚ ì²˜ë¦¬í•  íŒŒì¼ {len(changed_files)}ê°œ í™•ì¸")
        
        # GPT íŒ¨ì¹˜ ì‹¤í–‰
        patcher = GPTPatcher(api_key)
        result = patcher.call_gpt_api(instructions, changed_files)
        
        if not result:
            logger.error("âŒ GPT API í˜¸ì¶œ ì‹¤íŒ¨")
            return False
        
        # íŒŒì¼ ì‘ì—… ì‹¤í–‰
        if 'files' in result and result['files']:
            success = patcher.execute_file_operations(result['files'])
            
            if success:
                logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
                logger.info(f"ğŸ“‹ ì‘ì—… ìš”ì•½: {result.get('summary', 'N/A')}")
                return True
            else:
                logger.error("âŒ íŒŒì¼ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
                return False
        else:
            logger.warning("âš ï¸ ìˆ˜í–‰í•  íŒŒì¼ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤")
            return True
    
    except Exception as e:
        logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
